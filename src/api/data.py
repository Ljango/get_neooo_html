# -*- coding: utf-8 -*-
"""
æ•°æ®æ“ä½œAPI - æ•°æ®ç®¡ç†åŠŸèƒ½ï¼ˆrootå’Œengineerå¯ç”¨ï¼‰
"""

import subprocess
import sys
import shutil
import tarfile
import zipfile
import os
import json
import tempfile
from datetime import datetime
from typing import Optional, List, Dict, Any
from pathlib import Path
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, UploadFile, File, Form
from sqlalchemy.orm import Session

from .database import get_db
from .models import User, DataVersion, VersionStatus, SyncQueue
from .schemas import (
    TaskResponse, ResponseBase, VersionInfo, CreateVersionRequest
)
from .deps import require_root, require_engineer, log_operation
from config import SUBJECT_CONFIG, PROJECT_ROOT, DATA_ROOT

# å¿«ç…§å­˜å‚¨ç›®å½•
SNAPSHOT_DIR = PROJECT_ROOT / "archive" / "snapshots"

router = APIRouter()


def run_script(script_path: str, args: List[str] = None) -> tuple:
    """è¿è¡ŒPythonè„šæœ¬å¹¶è¿”å›è¾“å‡º"""
    cmd = [sys.executable, script_path]
    if args:
        cmd.extend(args)
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
            cwd=str(PROJECT_ROOT)
        )
        return result.returncode == 0, result.stdout + result.stderr
    except subprocess.TimeoutExpired:
        return False, "è„šæœ¬æ‰§è¡Œè¶…æ—¶"
    except Exception as e:
        return False, str(e)


# ========== æ•°æ®æ£€æŸ¥ä¸ä¿®å¤ ==========

@router.post("/normalize", response_model=TaskResponse)
async def run_normalize(
    subject: Optional[str] = None,
    fix: bool = False,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """æ‰§è¡Œæ•°æ®è§„èŒƒåŒ–æ£€æŸ¥/ä¿®å¤ï¼ˆä»…rootï¼‰"""
    script_path = PROJECT_ROOT / "scripts" / "data_normalizer.py"
    
    if not script_path.exists():
        raise HTTPException(status_code=500, detail="è§„èŒƒåŒ–è„šæœ¬ä¸å­˜åœ¨")
    
    args = ["--check" if not fix else "--fix"]
    if subject:
        args.extend(["--subject", subject])
    if fix:
        args.append("--no-backup")  # APIè°ƒç”¨ä¸è‡ªåŠ¨å¤‡ä»½ï¼Œç”±ç”¨æˆ·ç¡®è®¤
    
    success, output = run_script(str(script_path), args)
    
    log_operation(
        db, current_user, "data_normalize",
        details={"subject": subject, "fix": fix, "success": success}
    )
    
    return TaskResponse(
        success=success,
        message="æ•°æ®è§„èŒƒåŒ–å®Œæˆ" if success else "æ‰§è¡Œå¤±è´¥",
        output=output
    )


@router.post("/analyze", response_model=TaskResponse)
async def run_analyze(
    subject: Optional[str] = None,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """æ‰§è¡Œæ•°æ®è´¨é‡åˆ†æï¼ˆä»…rootï¼‰"""
    script_path = PROJECT_ROOT / "scripts" / "data_analyzer.py"
    
    if not script_path.exists():
        raise HTTPException(status_code=500, detail="åˆ†æè„šæœ¬ä¸å­˜åœ¨")
    
    args = ["--all"]
    if subject:
        args.extend(["--subject", subject])
    
    success, output = run_script(str(script_path), args)
    
    log_operation(
        db, current_user, "data_analyze",
        details={"subject": subject, "success": success}
    )
    
    return TaskResponse(
        success=success,
        message="æ•°æ®åˆ†æå®Œæˆ" if success else "æ‰§è¡Œå¤±è´¥",
        output=output
    )


@router.post("/generate-html", response_model=TaskResponse)
async def run_generate_html(
    subject: Optional[str] = None,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """ç”ŸæˆHTMLå›¾è°±ï¼ˆä»…rootï¼‰"""
    script_path = PROJECT_ROOT / "manage.py"
    
    args = ["generate"]
    if subject:
        args.extend(["--subject", subject])
    else:
        args.append("--all")
    
    success, output = run_script(str(script_path), args)
    
    # åŒæ—¶æ›´æ–°index
    if success:
        run_script(str(script_path), ["update-index"])
    
    log_operation(
        db, current_user, "generate_html",
        details={"subject": subject, "success": success}
    )
    
    return TaskResponse(
        success=success,
        message="HTMLç”Ÿæˆå®Œæˆ" if success else "æ‰§è¡Œå¤±è´¥",
        output=output
    )


@router.post("/import-neo4j", response_model=TaskResponse)
async def run_import_neo4j(
    subject: Optional[str] = None,
    target: str = "local",
    clear: bool = False,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """å¯¼å…¥æ•°æ®åˆ°Neo4jï¼ˆä»…rootï¼‰"""
    script_path = PROJECT_ROOT / "manage.py"
    
    args = ["import"]
    if subject:
        args.extend(["--subject", subject])
    else:
        args.append("--all")
    
    args.extend(["--target", target])
    
    if clear:
        args.append("--clear")
    
    success, output = run_script(str(script_path), args)
    
    log_operation(
        db, current_user, "import_neo4j",
        details={"subject": subject, "target": target, "clear": clear, "success": success}
    )
    
    return TaskResponse(
        success=success,
        message="Neo4jå¯¼å…¥å®Œæˆ" if success else "æ‰§è¡Œå¤±è´¥",
        output=output
    )


@router.post("/sync", response_model=TaskResponse)
async def run_sync(
    subject: Optional[str] = None,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """åŒæ­¥å­¦ç§‘æ•°æ®ï¼ˆç”ŸæˆHTML + æ›´æ–°ç´¢å¼•ï¼‰ï¼ˆä»…rootï¼‰"""
    script_path = PROJECT_ROOT / "manage.py"
    
    args = ["sync"]
    if subject:
        args.extend(["--subject", subject])
    else:
        args.append("--all")
    
    success, output = run_script(str(script_path), args)
    
    # åŒæ­¥æˆåŠŸåæ¸…é™¤needs_syncæ ‡è®°
    if success:
        if subject:
            sync_record = db.query(SyncQueue).filter(SyncQueue.subject_id == subject).first()
            if sync_record:
                sync_record.needs_sync = False
                sync_record.last_sync_at = datetime.utcnow()
                sync_record.edit_count = 0
                db.commit()
        else:
            # æ¸…é™¤æ‰€æœ‰å­¦ç§‘çš„æ ‡è®°
            db.query(SyncQueue).update({
                SyncQueue.needs_sync: False,
                SyncQueue.last_sync_at: datetime.utcnow(),
                SyncQueue.edit_count: 0
            })
            db.commit()
    
    log_operation(
        db, current_user, "sync_data",
        details={"subject": subject, "success": success}
    )
    
    return TaskResponse(
        success=success,
        message="æ•°æ®åŒæ­¥å®Œæˆ" if success else "æ‰§è¡Œå¤±è´¥",
        output=output
    )


# ========== åŒæ­¥çŠ¶æ€ ==========

@router.get("/sync-status")
async def get_sync_status(
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """è·å–æ‰€æœ‰å­¦ç§‘çš„åŒæ­¥çŠ¶æ€"""
    sync_records = db.query(SyncQueue).all()
    
    # æ„å»ºçŠ¶æ€å­—å…¸
    status_dict = {}
    for record in sync_records:
        status_dict[record.subject_id] = {
            "needs_sync": record.needs_sync,
            "last_edit_at": record.last_edit_at.isoformat() if record.last_edit_at else None,
            "last_sync_at": record.last_sync_at.isoformat() if record.last_sync_at else None,
            "edit_count": record.edit_count
        }
    
    # æ£€æŸ¥å“ªäº›å­¦ç§‘éœ€è¦åŒæ­¥
    subjects_need_sync = [
        {
            "subject_id": r.subject_id,
            "display_name": SUBJECT_CONFIG.get(r.subject_id, {}).get('display_name', r.subject_id),
            "edit_count": r.edit_count,
            "last_edit_at": r.last_edit_at.isoformat() if r.last_edit_at else None
        }
        for r in sync_records if r.needs_sync
    ]
    
    return {
        "success": True,
        "subjects_need_sync": subjects_need_sync,
        "total_need_sync": len(subjects_need_sync),
        "all_status": status_dict
    }


# ========== ç‰ˆæœ¬ç®¡ç† ==========

@router.get("/versions/{subject_id}")
async def list_versions(
    subject_id: str,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """è·å–å­¦ç§‘ç‰ˆæœ¬åˆ—è¡¨ï¼ˆrootï¼‰"""
    versions = db.query(DataVersion).filter(
        DataVersion.subject_id == subject_id
    ).order_by(DataVersion.created_at.desc()).all()
    
    return {
        "success": True,
        "versions": [VersionInfo.model_validate(v) for v in versions]
    }


@router.post("/versions", response_model=VersionInfo)
async def create_version(
    version_data: CreateVersionRequest,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºæ–°ç‰ˆæœ¬ï¼ˆä»…rootï¼‰"""
    # æ£€æŸ¥ç‰ˆæœ¬æ˜¯å¦å·²å­˜åœ¨
    existing = db.query(DataVersion).filter(
        DataVersion.subject_id == version_data.subject_id,
        DataVersion.version == version_data.version
    ).first()
    
    if existing:
        raise HTTPException(status_code=400, detail="ç‰ˆæœ¬å·²å­˜åœ¨")
    
    # è·å–å®ä½“å’Œå…³ç³»æ•°é‡
    from .review import load_entities_from_json, load_relations_from_json
    entities = load_entities_from_json(version_data.subject_id)
    relations = load_relations_from_json(version_data.subject_id)
    
    version = DataVersion(
        subject_id=version_data.subject_id,
        version=version_data.version,
        base_version=version_data.base_version,
        description=version_data.description,
        entity_count=len(entities),
        relation_count=len(relations),
        created_by=current_user.id
    )
    db.add(version)
    db.commit()
    db.refresh(version)
    
    log_operation(
        db, current_user, "create_version", "version", str(version.id),
        details={"subject_id": version_data.subject_id, "version": version_data.version}
    )
    
    return VersionInfo.model_validate(version)


@router.post("/versions/{version_id}/publish", response_model=VersionInfo)
async def publish_version(
    version_id: int,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """å‘å¸ƒç‰ˆæœ¬ï¼ˆä»…rootï¼‰"""
    version = db.query(DataVersion).filter(DataVersion.id == version_id).first()
    if not version:
        raise HTTPException(status_code=404, detail="ç‰ˆæœ¬ä¸å­˜åœ¨")
    
    version.status = VersionStatus.published
    version.published_at = datetime.utcnow()
    db.commit()
    db.refresh(version)
    
    log_operation(
        db, current_user, "publish_version", "version", str(version_id),
        details={"subject_id": version.subject_id, "version": version.version}
    )
    
    return VersionInfo.model_validate(version)


# ========== å­¦ç§‘åˆ—è¡¨ ==========

@router.get("/subjects")
async def list_all_subjects(
    current_user: User = Depends(require_root)
):
    """è·å–æ‰€æœ‰å­¦ç§‘åˆ—è¡¨ï¼ˆrootï¼‰"""
    subjects = []
    for name, config in SUBJECT_CONFIG.items():
        subjects.append({
            "subject_id": name,
            "display_name": config.get('display_name', name),
            "icon": config.get('icon', 'ğŸ“š'),
            "data_dir": config.get('data_dir'),
            "neo4j_label": config.get('neo4j_label')
        })
    
    return {"success": True, "subjects": subjects}


# ========== å¯¼å‡ºåŠŸèƒ½ ==========

@router.post("/export", response_model=TaskResponse)
async def export_data(
    subject: Optional[str] = None,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """å¯¼å‡ºæ•°æ®åˆ°Excelï¼ˆä»…rootï¼‰"""
    script_path = PROJECT_ROOT / "scripts" / "json2csv.py"
    
    if not script_path.exists():
        raise HTTPException(status_code=500, detail="å¯¼å‡ºè„šæœ¬ä¸å­˜åœ¨")
    
    args = []
    if subject:
        args.extend(["--subject", subject])
    
    success, output = run_script(str(script_path), args)
    
    log_operation(
        db, current_user, "export_data",
        details={"subject": subject, "success": success}
    )
    
    return TaskResponse(
        success=success,
        message="æ•°æ®å¯¼å‡ºå®Œæˆ" if success else "æ‰§è¡Œå¤±è´¥",
        output=output
    )


# ========== æ•°æ®å¿«ç…§ç®¡ç† ==========

@router.get("/snapshots/{subject_id}")
async def list_snapshots(
    subject_id: str,
    current_user: User = Depends(require_root)
):
    """è·å–å­¦ç§‘çš„æ‰€æœ‰å¿«ç…§åˆ—è¡¨"""
    subject_config = SUBJECT_CONFIG.get(subject_id)
    if not subject_config:
        raise HTTPException(status_code=404, detail="å­¦ç§‘ä¸å­˜åœ¨")
    
    # è·å–data_diråç§°ä½œä¸ºå¿«ç…§ç›®å½•å
    data_dir_name = Path(subject_config['data_dir']).name
    snapshot_path = SNAPSHOT_DIR / data_dir_name
    
    snapshots = []
    if snapshot_path.exists():
        for f in sorted(snapshot_path.glob("*.tar.gz"), reverse=True):
            stat = f.stat()
            # è§£ææ–‡ä»¶åè·å–ç‰ˆæœ¬å’Œæ—¶é—´
            # æ ¼å¼: v1.0_20260123_150000.tar.gz
            name_parts = f.stem.replace('.tar', '').split('_')
            version = name_parts[0] if name_parts else 'unknown'
            
            snapshots.append({
                "filename": f.name,
                "version": version,
                "size": stat.st_size,
                "size_human": f"{stat.st_size / 1024 / 1024:.2f} MB",
                "created_at": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "path": str(f)
            })
    
    return {
        "success": True,
        "subject_id": subject_id,
        "snapshots": snapshots,
        "total": len(snapshots)
    }


@router.post("/snapshots/create")
async def create_snapshot(
    subject_id: str,
    version: str = "auto",
    description: str = "",
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºå­¦ç§‘æ•°æ®å¿«ç…§"""
    subject_config = SUBJECT_CONFIG.get(subject_id)
    if not subject_config:
        raise HTTPException(status_code=404, detail="å­¦ç§‘ä¸å­˜åœ¨")
    
    data_dir = DATA_ROOT / subject_config['data_dir']
    if not data_dir.exists():
        raise HTTPException(status_code=404, detail="å­¦ç§‘æ•°æ®ç›®å½•ä¸å­˜åœ¨")
    
    # è·å–data_diråç§°ä½œä¸ºå¿«ç…§ç›®å½•å
    data_dir_name = Path(subject_config['data_dir']).name
    snapshot_path = SNAPSHOT_DIR / data_dir_name
    snapshot_path.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆç‰ˆæœ¬å·
    if version == "auto":
        existing = list(snapshot_path.glob("*.tar.gz"))
        version = f"v{len(existing) + 1}.0"
    
    # ç”Ÿæˆå¿«ç…§æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    snapshot_file = snapshot_path / f"{version}_{timestamp}.tar.gz"
    
    try:
        # ç»Ÿè®¡æ•°æ®
        entities_count = 0
        relations_count = 0
        
        entities_dir = data_dir / "entities"
        relations_dir = data_dir / "relations"
        
        if entities_dir.exists():
            for json_file in entities_dir.glob("*.json"):
                import json
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    entities_count += len(data) if isinstance(data, list) else len(data.get('entities', []))
        
        if relations_dir.exists():
            for json_file in relations_dir.glob("*.json"):
                import json
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    relations_count += len(data) if isinstance(data, list) else len(data.get('relations', []))
        
        # åˆ›å»ºtar.gzå½’æ¡£
        with tarfile.open(snapshot_file, "w:gz") as tar:
            # æ·»åŠ å…ƒæ•°æ®æ–‡ä»¶
            import json
            import tempfile
            metadata = {
                "subject_id": subject_id,
                "version": version,
                "description": description,
                "created_at": datetime.now().isoformat(),
                "created_by": current_user.username,
                "entities_count": entities_count,
                "relations_count": relations_count,
                "data_dir": str(data_dir)
            }
            
            # å†™å…¥metadata.json
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tf:
                json.dump(metadata, tf, ensure_ascii=False, indent=2)
                tf.flush()
                tar.add(tf.name, arcname="metadata.json")
                os.unlink(tf.name)
            
            # æ·»åŠ entitiesç›®å½•
            if entities_dir.exists():
                for json_file in entities_dir.glob("*.json"):
                    tar.add(json_file, arcname=f"entities/{json_file.name}")
            
            # æ·»åŠ relationsç›®å½•
            if relations_dir.exists():
                for json_file in relations_dir.glob("*.json"):
                    tar.add(json_file, arcname=f"relations/{json_file.name}")
        
        # è®°å½•æ“ä½œæ—¥å¿—
        log_operation(
            db, current_user, "create_snapshot", "snapshot", snapshot_file.name,
            details={
                "subject_id": subject_id,
                "version": version,
                "file": str(snapshot_file),
                "entities_count": entities_count,
                "relations_count": relations_count
            }
        )
        
        return {
            "success": True,
            "message": f"å¿«ç…§åˆ›å»ºæˆåŠŸ: {snapshot_file.name}",
            "snapshot": {
                "filename": snapshot_file.name,
                "version": version,
                "path": str(snapshot_file),
                "size": snapshot_file.stat().st_size,
                "entities_count": entities_count,
                "relations_count": relations_count
            }
        }
        
    except Exception as e:
        # æ¸…ç†å¤±è´¥çš„å¿«ç…§æ–‡ä»¶
        if snapshot_file.exists():
            snapshot_file.unlink()
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¿«ç…§å¤±è´¥: {str(e)}")


@router.post("/snapshots/restore")
async def restore_snapshot(
    subject_id: str,
    filename: str,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """ä»å¿«ç…§æ¢å¤å­¦ç§‘æ•°æ®"""
    subject_config = SUBJECT_CONFIG.get(subject_id)
    if not subject_config:
        raise HTTPException(status_code=404, detail="å­¦ç§‘ä¸å­˜åœ¨")
    
    data_dir = DATA_ROOT / subject_config['data_dir']
    data_dir_name = Path(subject_config['data_dir']).name
    snapshot_path = SNAPSHOT_DIR / data_dir_name / filename
    
    if not snapshot_path.exists():
        raise HTTPException(status_code=404, detail="å¿«ç…§æ–‡ä»¶ä¸å­˜åœ¨")
    
    try:
        # å…ˆåˆ›å»ºå½“å‰æ•°æ®çš„å¤‡ä»½
        backup_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = SNAPSHOT_DIR / data_dir_name / f"pre_restore_backup_{backup_timestamp}.tar.gz"
        
        with tarfile.open(backup_file, "w:gz") as tar:
            entities_dir = data_dir / "entities"
            relations_dir = data_dir / "relations"
            
            if entities_dir.exists():
                for json_file in entities_dir.glob("*.json"):
                    tar.add(json_file, arcname=f"entities/{json_file.name}")
            
            if relations_dir.exists():
                for json_file in relations_dir.glob("*.json"):
                    tar.add(json_file, arcname=f"relations/{json_file.name}")
        
        # è§£å‹å¿«ç…§è¦†ç›–å½“å‰æ•°æ®
        with tarfile.open(snapshot_path, "r:gz") as tar:
            # è¯»å–metadata
            metadata = None
            try:
                metadata_file = tar.extractfile("metadata.json")
                if metadata_file:
                    import json
                    metadata = json.load(metadata_file)
            except:
                pass
            
            # æ¸…ç©ºå¹¶æ¢å¤entitiesç›®å½•
            entities_dir = data_dir / "entities"
            if entities_dir.exists():
                for json_file in entities_dir.glob("*.json"):
                    json_file.unlink()
            else:
                entities_dir.mkdir(parents=True, exist_ok=True)
            
            # æ¸…ç©ºå¹¶æ¢å¤relationsç›®å½•
            relations_dir = data_dir / "relations"
            if relations_dir.exists():
                for json_file in relations_dir.glob("*.json"):
                    json_file.unlink()
            else:
                relations_dir.mkdir(parents=True, exist_ok=True)
            
            # è§£å‹æ–‡ä»¶
            for member in tar.getmembers():
                if member.name == "metadata.json":
                    continue
                
                if member.name.startswith("entities/"):
                    target = entities_dir / Path(member.name).name
                    content = tar.extractfile(member)
                    if content:
                        with open(target, 'wb') as f:
                            f.write(content.read())
                
                elif member.name.startswith("relations/"):
                    target = relations_dir / Path(member.name).name
                    content = tar.extractfile(member)
                    if content:
                        with open(target, 'wb') as f:
                            f.write(content.read())
        
        # è®°å½•æ“ä½œæ—¥å¿—
        log_operation(
            db, current_user, "restore_snapshot", "snapshot", filename,
            details={
                "subject_id": subject_id,
                "snapshot": filename,
                "backup": str(backup_file),
                "metadata": metadata
            }
        )
        
        return {
            "success": True,
            "message": f"æ•°æ®æ¢å¤æˆåŠŸï¼ŒåŸæ•°æ®å·²å¤‡ä»½åˆ°: {backup_file.name}",
            "backup_file": backup_file.name,
            "restored_from": filename,
            "metadata": metadata
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¢å¤å¤±è´¥: {str(e)}")


@router.delete("/snapshots/{subject_id}/{filename}")
async def delete_snapshot(
    subject_id: str,
    filename: str,
    current_user: User = Depends(require_root),
    db: Session = Depends(get_db)
):
    """åˆ é™¤å¿«ç…§"""
    subject_config = SUBJECT_CONFIG.get(subject_id)
    if not subject_config:
        raise HTTPException(status_code=404, detail="å­¦ç§‘ä¸å­˜åœ¨")
    
    data_dir_name = Path(subject_config['data_dir']).name
    snapshot_path = SNAPSHOT_DIR / data_dir_name / filename
    
    if not snapshot_path.exists():
        raise HTTPException(status_code=404, detail="å¿«ç…§æ–‡ä»¶ä¸å­˜åœ¨")
    
    try:
        snapshot_path.unlink()
        
        log_operation(
            db, current_user, "delete_snapshot", "snapshot", filename,
            details={"subject_id": subject_id}
        )
        
        return {
            "success": True,
            "message": f"å¿«ç…§å·²åˆ é™¤: {filename}"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


# ========== ZIPæ•°æ®åŒ…ä¸Šä¼  ==========

# å®ä½“å¿…éœ€å­—æ®µ
ENTITY_REQUIRED_FIELDS = ['identifier', 'title', 'type']
# å…³ç³»å¿…éœ€å­—æ®µ
RELATION_REQUIRED_FIELDS = ['source', 'target', 'relationName']


def is_pascal_case(s: str) -> bool:
    """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºPascalCaseæ ¼å¼"""
    if not s or len(s) < 2:
        return True
    # PascalCase: é¦–å­—æ¯å¤§å†™ï¼Œä¸å…¨æ˜¯å¤§å†™ï¼Œä¸åŒ…å«ä¸‹åˆ’çº¿å¼€å¤´
    return s[0].isupper() and not s.isupper() and not s.startswith('_')


def validate_entity_json(data: Any, filename: str) -> Dict[str, Any]:
    """
    éªŒè¯å®ä½“JSONæ•°æ®æ ¼å¼
    
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. åˆ—è¡¨æ ¼å¼: [{"identifier": ..., "title": ..., "type": ...}, ...]
    2. å¯¹è±¡æ ¼å¼: {"entities": [...]}
    
    å¢å¼ºæ£€æŸ¥ï¼š
    - æ–‡ä»¶å‘½åè§„èŒƒï¼ˆPascalCaseï¼‰
    - typeå­—æ®µä¸æ–‡ä»¶åä¸€è‡´æ€§
    - identifieræ ¼å¼è§„èŒƒ
    """
    result = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "count": 0
    }
    
    # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºæœŸæœ›çš„type
    expected_type = Path(filename).stem
    
    # æ£€æŸ¥æ–‡ä»¶å‘½åæ˜¯å¦ä¸ºPascalCaseï¼ˆä»…è­¦å‘Šï¼‰
    if expected_type and not is_pascal_case(expected_type):
        # æ£€æŸ¥æ˜¯å¦å…¨å°å†™
        if expected_type.islower():
            result["warnings"].append(f"{filename}: æ–‡ä»¶åå»ºè®®ä½¿ç”¨PascalCaseæ ¼å¼ï¼ˆå¦‚ {expected_type.title()}.jsonï¼‰")
    
    # è·å–å®ä½“åˆ—è¡¨
    if isinstance(data, list):
        entities = data
    elif isinstance(data, dict) and 'entities' in data:
        entities = data['entities']
    else:
        result["valid"] = False
        result["errors"].append(f"{filename}: æ ¼å¼é”™è¯¯ï¼Œéœ€è¦æ˜¯åˆ—è¡¨æˆ–åŒ…å«'entities'å­—æ®µçš„å¯¹è±¡")
        return result
    
    if not isinstance(entities, list):
        result["valid"] = False
        result["errors"].append(f"{filename}: 'entities'å­—æ®µå¿…é¡»æ˜¯åˆ—è¡¨")
        return result
    
    result["count"] = len(entities)
    
    if len(entities) == 0:
        result["warnings"].append(f"{filename}: å®ä½“åˆ—è¡¨ä¸ºç©º")
        return result
    
    # æ£€æŸ¥æ¯ä¸ªå®ä½“çš„å¿…éœ€å­—æ®µ
    identifiers = set()
    type_mismatch_count = 0
    identifier_case_warnings = 0
    
    for i, entity in enumerate(entities):
        if not isinstance(entity, dict):
            result["valid"] = False
            result["errors"].append(f"{filename}[{i}]: å®ä½“å¿…é¡»æ˜¯å¯¹è±¡")
            continue
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        missing = [f for f in ENTITY_REQUIRED_FIELDS if f not in entity or not entity[f]]
        if missing:
            result["valid"] = False
            result["errors"].append(f"{filename}[{i}]: ç¼ºå°‘å¿…éœ€å­—æ®µ {missing}")
        
        # æ£€æŸ¥typeå­—æ®µä¸æ–‡ä»¶åä¸€è‡´æ€§
        entity_type = entity.get('type', '')
        if entity_type and entity_type != expected_type:
            type_mismatch_count += 1
        
        # æ£€æŸ¥identifieræ ¼å¼
        identifier = entity.get('identifier', '')
        if identifier:
            # æ£€æŸ¥å”¯ä¸€æ€§
            if identifier in identifiers:
                result["warnings"].append(f"{filename}[{i}]: identifieré‡å¤: {identifier}")
            identifiers.add(identifier)
            
            # æ£€æŸ¥identifierå‰ç¼€æ˜¯å¦ä¸ºPascalCase
            if '_' in identifier:
                prefix = identifier.split('_')[0]
                if prefix.islower() and len(prefix) > 2:
                    identifier_case_warnings += 1
    
    # æ±‡æ€»è­¦å‘Šï¼ˆé¿å…è¿‡å¤šé‡å¤è­¦å‘Šï¼‰
    if type_mismatch_count > 0:
        result["warnings"].append(
            f"{filename}: {type_mismatch_count}ä¸ªå®ä½“çš„typeå­—æ®µä¸æ–‡ä»¶å({expected_type})ä¸ä¸€è‡´"
        )
    
    if identifier_case_warnings > 0:
        result["warnings"].append(
            f"{filename}: {identifier_case_warnings}ä¸ªå®ä½“çš„identifierå‰ç¼€å»ºè®®ä½¿ç”¨PascalCaseæ ¼å¼"
        )
    
    return result


def validate_relation_json(data: Any, filename: str) -> Dict[str, Any]:
    """
    éªŒè¯å…³ç³»JSONæ•°æ®æ ¼å¼
    
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. åˆ—è¡¨æ ¼å¼: [{"source": ..., "target": ..., "relationName": ...}, ...]
    2. å¯¹è±¡æ ¼å¼: {"relations": [...]}
    """
    result = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "count": 0
    }
    
    # è·å–å…³ç³»åˆ—è¡¨
    if isinstance(data, list):
        relations = data
    elif isinstance(data, dict) and 'relations' in data:
        relations = data['relations']
    else:
        result["valid"] = False
        result["errors"].append(f"{filename}: æ ¼å¼é”™è¯¯ï¼Œéœ€è¦æ˜¯åˆ—è¡¨æˆ–åŒ…å«'relations'å­—æ®µçš„å¯¹è±¡")
        return result
    
    if not isinstance(relations, list):
        result["valid"] = False
        result["errors"].append(f"{filename}: 'relations'å­—æ®µå¿…é¡»æ˜¯åˆ—è¡¨")
        return result
    
    result["count"] = len(relations)
    
    if len(relations) == 0:
        result["warnings"].append(f"{filename}: å…³ç³»åˆ—è¡¨ä¸ºç©º")
        return result
    
    # æ£€æŸ¥æ¯ä¸ªå…³ç³»çš„å¿…éœ€å­—æ®µ
    for i, relation in enumerate(relations):
        if not isinstance(relation, dict):
            result["valid"] = False
            result["errors"].append(f"{filename}[{i}]: å…³ç³»å¿…é¡»æ˜¯å¯¹è±¡")
            continue
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        missing = [f for f in RELATION_REQUIRED_FIELDS if f not in relation or not relation[f]]
        if missing:
            result["valid"] = False
            result["errors"].append(f"{filename}[{i}]: ç¼ºå°‘å¿…éœ€å­—æ®µ {missing}")
    
    return result


def validate_cross_file_references(
    entity_identifiers: set,
    relations_data: List[Dict],
    relation_filename: str
) -> Dict[str, Any]:
    """
    éªŒè¯å…³ç³»æ–‡ä»¶ä¸­çš„å¼•ç”¨æ˜¯å¦åœ¨å®ä½“ä¸­å­˜åœ¨
    
    æ£€æŸ¥ï¼š
    - source/target æ˜¯å¦åœ¨å®ä½“identifiersä¸­å­˜åœ¨
    - å¤§å°å†™å˜ä½“å†²çªæ£€æµ‹
    """
    result = {
        "warnings": [],
        "missing_refs": [],
        "case_mismatches": []
    }
    
    # åˆ›å»ºå°å†™åˆ°åŸå§‹identifierçš„æ˜ å°„
    id_lower_map = {eid.lower(): eid for eid in entity_identifiers}
    
    missing_sources = set()
    missing_targets = set()
    case_mismatch_sources = {}
    case_mismatch_targets = {}
    
    for relation in relations_data:
        if not isinstance(relation, dict):
            continue
            
        source = relation.get('source', '')
        target = relation.get('target', '')
        
        # æ£€æŸ¥source
        if source and source not in entity_identifiers:
            source_lower = source.lower()
            if source_lower in id_lower_map:
                # å¤§å°å†™ä¸åŒ¹é…
                if source not in case_mismatch_sources:
                    case_mismatch_sources[source] = id_lower_map[source_lower]
            else:
                missing_sources.add(source)
        
        # æ£€æŸ¥target
        if target and target not in entity_identifiers:
            target_lower = target.lower()
            if target_lower in id_lower_map:
                # å¤§å°å†™ä¸åŒ¹é…
                if target not in case_mismatch_targets:
                    case_mismatch_targets[target] = id_lower_map[target_lower]
            else:
                missing_targets.add(target)
    
    # ç”Ÿæˆè­¦å‘Š
    if case_mismatch_sources:
        for wrong, correct in list(case_mismatch_sources.items())[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            result["warnings"].append(
                f"{relation_filename}: sourceå¼•ç”¨'{wrong}'ä¸å®ä½“identifier'{correct}'å¤§å°å†™ä¸ä¸€è‡´"
            )
        if len(case_mismatch_sources) > 5:
            result["warnings"].append(
                f"{relation_filename}: è¿˜æœ‰{len(case_mismatch_sources)-5}ä¸ªsourceå¤§å°å†™ä¸ä¸€è‡´çš„é—®é¢˜"
            )
        result["case_mismatches"].extend(case_mismatch_sources.keys())
    
    if case_mismatch_targets:
        for wrong, correct in list(case_mismatch_targets.items())[:5]:
            result["warnings"].append(
                f"{relation_filename}: targetå¼•ç”¨'{wrong}'ä¸å®ä½“identifier'{correct}'å¤§å°å†™ä¸ä¸€è‡´"
            )
        if len(case_mismatch_targets) > 5:
            result["warnings"].append(
                f"{relation_filename}: è¿˜æœ‰{len(case_mismatch_targets)-5}ä¸ªtargetå¤§å°å†™ä¸ä¸€è‡´çš„é—®é¢˜"
            )
        result["case_mismatches"].extend(case_mismatch_targets.keys())
    
    # ç¼ºå¤±å¼•ç”¨ï¼ˆä»…è­¦å‘Šï¼Œä¸é˜»æ­¢ä¸Šä¼ ï¼‰
    if missing_sources:
        samples = list(missing_sources)[:3]
        result["warnings"].append(
            f"{relation_filename}: {len(missing_sources)}ä¸ªsourceå¼•ç”¨åœ¨å®ä½“ä¸­ä¸å­˜åœ¨ï¼Œå¦‚: {samples}"
        )
        result["missing_refs"].extend(missing_sources)
    
    if missing_targets:
        samples = list(missing_targets)[:3]
        result["warnings"].append(
            f"{relation_filename}: {len(missing_targets)}ä¸ªtargetå¼•ç”¨åœ¨å®ä½“ä¸­ä¸å­˜åœ¨ï¼Œå¦‚: {samples}"
        )
        result["missing_refs"].extend(missing_targets)
    
    return result


def validate_zip_structure(zip_path: Path) -> Dict[str, Any]:
    """
    éªŒè¯ZIPåŒ…ç»“æ„å’Œå†…å®¹
    
    é¢„æœŸç»“æ„:
    xxx.zip
    â”œâ”€â”€ entities/
    â”‚   â”œâ”€â”€ Type1.json
    â”‚   â””â”€â”€ Type2.json
    â””â”€â”€ relations/
        â”œâ”€â”€ Relation1.json
        â””â”€â”€ Relation2.json
    """
    result = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "entity_files": [],
        "relation_files": [],
        "total_entities": 0,
        "total_relations": 0
    }
    
    try:
        with zipfile.ZipFile(zip_path, 'r') as zf:
            file_list = zf.namelist()
            
            # æ£€æŸ¥ç›®å½•ç»“æ„
            has_entities_dir = any(f.startswith('entities/') for f in file_list)
            has_relations_dir = any(f.startswith('relations/') for f in file_list)
            
            if not has_entities_dir and not has_relations_dir:
                # å°è¯•æ£€æŸ¥æ˜¯å¦æœ‰å­ç›®å½•ï¼ˆå¦‚ subject-name/entities/ï¼‰
                # è¿‡æ»¤æ‰ __MACOSX å’Œéšè—ç›®å½•ï¼ˆä»¥.å¼€å¤´ï¼‰
                top_dirs = set(
                    f.split('/')[0] for f in file_list 
                    if '/' in f 
                    and not f.startswith('__MACOSX')
                    and not f.startswith('.')
                )
                if len(top_dirs) == 1:
                    top_dir = list(top_dirs)[0]
                    has_entities_dir = any(f.startswith(f'{top_dir}/entities/') for f in file_list)
                    has_relations_dir = any(f.startswith(f'{top_dir}/relations/') for f in file_list)
                    
                    if has_entities_dir or has_relations_dir:
                        result["warnings"].append(f"ZIPåŒ…å«å­ç›®å½• '{top_dir}/'ï¼Œå°†è‡ªåŠ¨å¤„ç†")
                        result["top_dir"] = top_dir
            
            if not has_entities_dir and not has_relations_dir:
                result["valid"] = False
                result["errors"].append("ZIPåŒ…å¿…é¡»åŒ…å« entities/ æˆ– relations/ ç›®å½•")
                return result
            
            # è·å–å‰ç¼€
            prefix = result.get("top_dir", "")
            if prefix:
                prefix = f"{prefix}/"
            
            # æ”¶é›†æ‰€æœ‰å®ä½“çš„identifierç”¨äºäº¤å‰éªŒè¯
            all_entity_identifiers = set()
            # æ”¶é›†å…³ç³»æ•°æ®ç”¨äºäº¤å‰éªŒè¯
            relations_for_validation = []  # [(filename, relations_list), ...]
            
            # éªŒè¯å®ä½“æ–‡ä»¶
            for f in file_list:
                if f.startswith(f'{prefix}entities/') and f.endswith('.json'):
                    filename = Path(f).name
                    try:
                        with zf.open(f) as entity_file:
                            data = json.load(entity_file)
                            validation = validate_entity_json(data, filename)
                            
                            if not validation["valid"]:
                                result["valid"] = False
                                result["errors"].extend(validation["errors"])
                            result["warnings"].extend(validation["warnings"])
                            result["entity_files"].append({
                                "path": f,
                                "filename": filename,
                                "count": validation["count"]
                            })
                            result["total_entities"] += validation["count"]
                            
                            # æ”¶é›†identifierç”¨äºäº¤å‰éªŒè¯
                            entities = data if isinstance(data, list) else data.get('entities', [])
                            for entity in entities:
                                if isinstance(entity, dict) and 'identifier' in entity:
                                    all_entity_identifiers.add(entity['identifier'])
                    except json.JSONDecodeError as e:
                        result["valid"] = False
                        result["errors"].append(f"entities/{filename}: JSONè§£æå¤±è´¥ - {str(e)}")
            
            # éªŒè¯å…³ç³»æ–‡ä»¶
            for f in file_list:
                if f.startswith(f'{prefix}relations/') and f.endswith('.json'):
                    filename = Path(f).name
                    try:
                        with zf.open(f) as relation_file:
                            data = json.load(relation_file)
                            validation = validate_relation_json(data, filename)
                            
                            if not validation["valid"]:
                                result["valid"] = False
                                result["errors"].extend(validation["errors"])
                            result["warnings"].extend(validation["warnings"])
                            result["relation_files"].append({
                                "path": f,
                                "filename": filename,
                                "count": validation["count"]
                            })
                            result["total_relations"] += validation["count"]
                            
                            # æ”¶é›†å…³ç³»æ•°æ®ç”¨äºäº¤å‰éªŒè¯
                            relations = data if isinstance(data, list) else data.get('relations', [])
                            relations_for_validation.append((filename, relations))
                    except json.JSONDecodeError as e:
                        result["valid"] = False
                        result["errors"].append(f"relations/{filename}: JSONè§£æå¤±è´¥ - {str(e)}")
            
            # æ‰§è¡Œäº¤å‰æ–‡ä»¶å¼•ç”¨éªŒè¯
            if all_entity_identifiers and relations_for_validation:
                for rel_filename, relations in relations_for_validation:
                    cross_validation = validate_cross_file_references(
                        all_entity_identifiers, relations, rel_filename
                    )
                    result["warnings"].extend(cross_validation["warnings"])
            
            if not result["entity_files"]:
                result["warnings"].append("æœªæ‰¾åˆ°å®ä½“æ–‡ä»¶")
            if not result["relation_files"]:
                result["warnings"].append("æœªæ‰¾åˆ°å…³ç³»æ–‡ä»¶")
                
    except zipfile.BadZipFile:
        result["valid"] = False
        result["errors"].append("æ— æ•ˆçš„ZIPæ–‡ä»¶")
    except Exception as e:
        result["valid"] = False
        result["errors"].append(f"éªŒè¯å¤±è´¥: {str(e)}")
    
    return result


@router.post("/upload/validate")
async def validate_upload(
    file: UploadFile = File(...),
    current_user: User = Depends(require_engineer)
):
    """
    éªŒè¯ä¸Šä¼ çš„ZIPæ•°æ®åŒ…æ ¼å¼ï¼ˆä¸å¯¼å…¥æ•°æ®ï¼‰
    
    å…è®¸è§’è‰²ï¼šroot, admin, engineer
    """
    if not file.filename.endswith('.zip'):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒZIPæ ¼å¼æ–‡ä»¶")
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = Path(tmp.name)
    
    try:
        # éªŒè¯ZIPç»“æ„
        validation = validate_zip_structure(tmp_path)
        
        return {
            "success": True,
            "filename": file.filename,
            "file_size": len(content),
            "validation": validation
        }
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        tmp_path.unlink(missing_ok=True)


@router.post("/upload")
async def upload_data_package(
    subject_id: str = Form(...),
    file: UploadFile = File(...),
    backup: bool = Form(True),
    current_user: User = Depends(require_engineer),
    db: Session = Depends(get_db)
):
    """
    ä¸Šä¼ å¹¶å¯¼å…¥ZIPæ•°æ®åŒ…
    
    å…è®¸è§’è‰²ï¼šroot, admin, engineer
    
    å‚æ•°ï¼š
    - subject_id: ç›®æ ‡å­¦ç§‘ID
    - file: ZIPæ–‡ä»¶
    - backup: æ˜¯å¦åœ¨å¯¼å…¥å‰åˆ›å»ºå¤‡ä»½ï¼ˆé»˜è®¤Trueï¼‰
    
    ZIPåŒ…æ ¼å¼è¦æ±‚ï¼š
    - å¿…é¡»åŒ…å« entities/ æˆ– relations/ ç›®å½•
    - å®ä½“JSONå¿…éœ€å­—æ®µ: identifier, title, type
    - å…³ç³»JSONå¿…éœ€å­—æ®µ: source, target, relationName
    """
    if not file.filename.endswith('.zip'):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒZIPæ ¼å¼æ–‡ä»¶")
    
    # æ£€æŸ¥å­¦ç§‘æ˜¯å¦å­˜åœ¨
    subject_config = SUBJECT_CONFIG.get(subject_id)
    if not subject_config:
        raise HTTPException(status_code=404, detail=f"å­¦ç§‘ä¸å­˜åœ¨: {subject_id}")
    
    data_dir = DATA_ROOT / subject_config['data_dir']
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = Path(tmp.name)
    
    try:
        # éªŒè¯ZIPç»“æ„
        validation = validate_zip_structure(tmp_path)
        
        if not validation["valid"]:
            return {
                "success": False,
                "message": "æ•°æ®åŒ…éªŒè¯å¤±è´¥",
                "errors": validation["errors"],
                "warnings": validation["warnings"]
            }
        
        # åˆ›å»ºå¤‡ä»½ï¼ˆå¦‚æœéœ€è¦ï¼‰
        backup_file = None
        if backup and data_dir.exists():
            data_dir_name = Path(subject_config['data_dir']).name
            backup_dir = SNAPSHOT_DIR / data_dir_name
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = backup_dir / f"pre_upload_backup_{timestamp}.tar.gz"
            
            with tarfile.open(backup_file, "w:gz") as tar:
                entities_dir = data_dir / "entities"
                relations_dir = data_dir / "relations"
                
                if entities_dir.exists():
                    for json_file in entities_dir.glob("*.json"):
                        tar.add(json_file, arcname=f"entities/{json_file.name}")
                
                if relations_dir.exists():
                    for json_file in relations_dir.glob("*.json"):
                        tar.add(json_file, arcname=f"relations/{json_file.name}")
        
        # è§£å‹å¹¶å¯¼å…¥æ•°æ®
        prefix = validation.get("top_dir", "")
        if prefix:
            prefix = f"{prefix}/"
        
        entities_dir = data_dir / "entities"
        relations_dir = data_dir / "relations"
        entities_dir.mkdir(parents=True, exist_ok=True)
        relations_dir.mkdir(parents=True, exist_ok=True)
        
        imported_files = {"entities": [], "relations": []}
        
        with zipfile.ZipFile(tmp_path, 'r') as zf:
            # å¯¼å…¥å®ä½“æ–‡ä»¶
            for ef in validation["entity_files"]:
                with zf.open(ef["path"]) as src:
                    target_path = entities_dir / ef["filename"]
                    with open(target_path, 'wb') as dst:
                        dst.write(src.read())
                    imported_files["entities"].append(ef["filename"])
            
            # å¯¼å…¥å…³ç³»æ–‡ä»¶
            for rf in validation["relation_files"]:
                with zf.open(rf["path"]) as src:
                    target_path = relations_dir / rf["filename"]
                    with open(target_path, 'wb') as dst:
                        dst.write(src.read())
                    imported_files["relations"].append(rf["filename"])
        
        # æ ‡è®°éœ€è¦åŒæ­¥HTML
        sync_record = db.query(SyncQueue).filter(SyncQueue.subject_id == subject_id).first()
        if sync_record:
            sync_record.needs_sync = True
            sync_record.last_edit_at = datetime.utcnow()
            sync_record.edit_count += 1
        else:
            sync_record = SyncQueue(
                subject_id=subject_id,
                needs_sync=True,
                last_edit_at=datetime.utcnow(),
                edit_count=1
            )
            db.add(sync_record)
        db.commit()
        
        # è®°å½•æ“ä½œæ—¥å¿—
        log_operation(
            db, current_user, "upload_data_package", "subject", subject_id,
            details={
                "filename": file.filename,
                "file_size": len(content),
                "entity_files": len(imported_files["entities"]),
                "relation_files": len(imported_files["relations"]),
                "total_entities": validation["total_entities"],
                "total_relations": validation["total_relations"],
                "backup": str(backup_file) if backup_file else None,
                "warnings": validation["warnings"]
            }
        )
        
        return {
            "success": True,
            "message": f"æ•°æ®åŒ…å¯¼å…¥æˆåŠŸï¼Œå…±å¯¼å…¥ {validation['total_entities']} ä¸ªå®ä½“å’Œ {validation['total_relations']} ä¸ªå…³ç³»",
            "subject_id": subject_id,
            "imported": {
                "entity_files": imported_files["entities"],
                "relation_files": imported_files["relations"],
                "total_entities": validation["total_entities"],
                "total_relations": validation["total_relations"]
            },
            "backup": str(backup_file) if backup_file else None,
            "warnings": validation["warnings"],
            "needs_sync": True
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥å¤±è´¥: {str(e)}")
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        tmp_path.unlink(missing_ok=True)


@router.get("/upload/format-spec")
async def get_upload_format_spec(
    current_user: User = Depends(require_engineer)
):
    """
    è·å–æ•°æ®åŒ…æ ¼å¼è§„èŒƒè¯´æ˜
    
    å…è®¸è§’è‰²ï¼šroot, admin, engineer
    """
    return {
        "success": True,
        "format_spec": {
            "description": "çŸ¥è¯†å›¾è°±æ•°æ®åŒ…æ ¼å¼è§„èŒƒ",
            "structure": {
                "zip_structure": "subject-name.zip\nâ”œâ”€â”€ entities/\nâ”‚   â”œâ”€â”€ Type1.json\nâ”‚   â””â”€â”€ Type2.json\nâ””â”€â”€ relations/\n    â”œâ”€â”€ Relation1.json\n    â””â”€â”€ Relation2.json",
                "notes": [
                    "ZIPåŒ…å¿…é¡»åŒ…å« entities/ æˆ– relations/ ç›®å½•",
                    "ä¹Ÿæ”¯æŒåŒ…å«å­ç›®å½•çš„æ ¼å¼ï¼Œå¦‚ subject-name/entities/",
                    "JSONæ–‡ä»¶å‘½åå»ºè®®ä½¿ç”¨å®ä½“ç±»å‹åï¼Œå¦‚ Chapter.json, Section.json"
                ]
            },
            "entity_format": {
                "description": "å®ä½“JSONæ ¼å¼",
                "required_fields": ENTITY_REQUIRED_FIELDS,
                "optional_fields": ["description", "subject", "applicableLevel", "contentJson"],
                "formats": [
                    {
                        "name": "åˆ—è¡¨æ ¼å¼",
                        "example": '[{"identifier": "urn:xxx", "title": "æ ‡é¢˜", "type": "Chapter"}]'
                    },
                    {
                        "name": "å¯¹è±¡æ ¼å¼",
                        "example": '{"entities": [{"identifier": "urn:xxx", "title": "æ ‡é¢˜", "type": "Chapter"}]}'
                    }
                ]
            },
            "relation_format": {
                "description": "å…³ç³»JSONæ ¼å¼",
                "required_fields": RELATION_REQUIRED_FIELDS,
                "optional_fields": ["label", "evidence"],
                "formats": [
                    {
                        "name": "åˆ—è¡¨æ ¼å¼",
                        "example": '[{"source": "urn:a", "target": "urn:b", "relationName": "hasChild"}]'
                    },
                    {
                        "name": "å¯¹è±¡æ ¼å¼",
                        "example": '{"relations": [{"source": "urn:a", "target": "urn:b", "relationName": "hasChild"}]}'
                    }
                ]
            },
            "available_subjects": [
                {"id": k, "name": v.get("display_name", k)}
                for k, v in SUBJECT_CONFIG.items()
            ]
        }
    }
