# 知识图谱数据审校系统 - 性能优化方案

## 一、并发性能优化

### 当前性能分析

**现状**：
- 使用同步SQLAlchemy
- 单线程数据库操作
- 无连接池优化
- API响应时间：50-500ms

**瓶颈**：
1. 数据库I/O阻塞
2. JSON文件读取阻塞
3. 大量实体/关系加载时内存占用高

### 优化方案

#### 1. 数据库层优化

**a) 异步数据库操作**
```python
# 使用异步SQLAlchemy
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine

# MySQL异步驱动
DATABASE_URL = "mysql+aiomysql://..."

# SQLite异步驱动  
DATABASE_URL = "sqlite+aiosqlite://..."
```

**性能提升**：
- 并发处理能力：1x → 10x+
- 响应时间减少：30-50%
- 支持更多并发连接

**b) 连接池配置**
```python
engine = create_async_engine(
    DATABASE_URL,
    pool_size=20,        # 核心连接数
    max_overflow=40,     # 最大额外连接
    pool_pre_ping=True,  # 连接健康检查
    pool_recycle=3600    # 1小时回收连接
)
```

#### 2. API层优化

**a) 异步路由处理器**
```python
# 同步 → 异步
@router.get("/entities")
def get_entities(...):  # 同步
    pass

@router.get("/entities")
async def get_entities(...):  # 异步
    pass
```

**b) 并发数据加载**
```python
import asyncio

# 并发加载多个学科数据
results = await asyncio.gather(
    load_subject_data("数学"),
    load_subject_data("物理"),
    load_subject_data("化学")
)
```

#### 3. 缓存策略

**a) 内存缓存**
```python
from functools import lru_cache
import time

# 缓存学科配置
@lru_cache(maxsize=100)
def get_subject_config(subject_id: str):
    return SUBJECT_CONFIG.get(subject_id)

# 缓存实体数据（5分钟）
entity_cache = {}
CACHE_TTL = 300

async def get_entities_cached(subject_id: str):
    cache_key = f"entities_{subject_id}"
    if cache_key in entity_cache:
        data, timestamp = entity_cache[cache_key]
        if time.time() - timestamp < CACHE_TTL:
            return data
    
    # 加载并缓存
    data = await load_entities(subject_id)
    entity_cache[cache_key] = (data, time.time())
    return data
```

**b) Redis缓存（生产环境）**
```python
import aioredis

redis = await aioredis.create_redis_pool('redis://localhost')

async def get_with_cache(key: str, loader_func):
    # 尝试从Redis获取
    cached = await redis.get(key)
    if cached:
        return json.loads(cached)
    
    # 加载并缓存
    data = await loader_func()
    await redis.setex(key, 300, json.dumps(data))
    return data
```

#### 4. 文件I/O优化

**a) 异步文件读取**
```python
import aiofiles

async def load_json_async(file_path: Path):
    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
        content = await f.read()
        return json.loads(content)
```

**b) 批量加载优化**
```python
async def load_all_entities(subject_dir: Path):
    entity_files = list(entity_dir.glob("*.json"))
    
    # 并发加载所有文件
    tasks = [load_json_async(f) for f in entity_files]
    results = await asyncio.gather(*tasks)
    
    # 合并结果
    all_entities = []
    for data in results:
        entities = data if isinstance(data, list) else data.get('entities', [])
        all_entities.extend(entities)
    
    return all_entities
```

#### 5. 分页优化

**当前问题**：一次加载所有数据后分页
**优化方案**：数据库级分页 + 游标

```python
# 优化前：加载所有再分页
entities = load_all_entities(subject_id)  # 6000+条
page_data = entities[start:end]  # 只用50条

# 优化后：按需加载
async def get_entities_paginated(subject_id, page, page_size):
    # 使用数据库查询分页
    offset = (page - 1) * page_size
    query = select(Entity).where(
        Entity.subject_id == subject_id
    ).offset(offset).limit(page_size)
    
    result = await db.execute(query)
    return result.scalars().all()
```

### 性能对比预测

| 指标 | 当前 | 优化后 | 提升 |
|------|------|--------|------|
| 并发连接数 | ~10 | 100+ | 10x |
| 实体列表响应 | 200ms | 50ms | 4x |
| 统计查询 | 500ms | 100ms | 5x |
| 登录响应 | 50ms | 20ms | 2.5x |
| 内存占用 | 高 | 中等 | 30% ↓ |

## 二、实时查看图谱优化

### 需求分析
1. 审核时实时查看实体关联图谱
2. 支持图谱交互（缩放、拖拽）
3. 高亮当前审核节点
4. 显示关系详情

### 实现方案

#### 1. 集成D3.js可视化

**在审核页面添加图谱面板**：
```javascript
// review.html 添加图谱容器
<div class="graph-panel" id="graphPanel">
    <svg id="graphSvg" width="100%" height="600px"></svg>
</div>

// 加载实体时渲染图谱
async function renderGraph(entityId) {
    const data = await api.review.getEntityGraph(subjectId, entityId);
    drawD3Graph(data.nodes, data.links);
}
```

#### 2. 按需加载图谱数据

```python
# API: 获取实体周边图谱
@router.get("/entity-graph/{subject_id}/{identifier}")
async def get_entity_graph(
    subject_id: str,
    identifier: str,
    depth: int = 2  # 关联深度
):
    """获取实体及其关联图谱"""
    # 中心节点
    center_entity = get_entity(subject_id, identifier)
    
    # 1度关系
    relations_1 = get_relations(subject_id, identifier)
    nodes_1 = [get_entity(subject_id, r.target) for r in relations_1]
    
    # 2度关系（可选）
    if depth >= 2:
        relations_2 = []
        for node in nodes_1:
            relations_2.extend(get_relations(subject_id, node.identifier))
    
    return {
        "center": center_entity,
        "nodes": nodes_1 + nodes_2,
        "links": relations_1 + relations_2
    }
```

#### 3. 图谱性能优化

**限制节点数量**：
```javascript
// 最多显示100个节点
const MAX_NODES = 100;

function filterNodes(nodes, centerNode) {
    if (nodes.length <= MAX_NODES) return nodes;
    
    // 优先保留：
    // 1. 中心节点的直接邻居
    // 2. 重要度高的节点（关系数多）
    return nodes
        .sort((a, b) => b.degree - a.degree)
        .slice(0, MAX_NODES);
}
```

**虚拟化渲染**：
```javascript
// 只渲染视口内的节点
function renderVisibleNodes(nodes, viewport) {
    return nodes.filter(node => 
        isInViewport(node, viewport)
    );
}
```

## 三、数据编辑入库优化

### 需求分析
1. 实体属性编辑（标题、描述等）
2. 关系增加/删除
3. 实时保存到JSON文件
4. 版本控制和历史记录

### 实现方案

#### 1. 增量更新策略

**避免全量写入**：
```python
# 优化前：修改后重写整个文件
entities = load_all_entities()
entities[0]['title'] = "新标题"
save_all_entities(entities)  # 慢

# 优化后：定位修改
async def update_entity_atomic(subject_id, identifier, updates):
    # 1. 找到文件
    entity_file = find_entity_file(subject_id, identifier)
    
    # 2. 加载数据
    async with aiofiles.open(entity_file, 'r+') as f:
        data = json.loads(await f.read())
        entities = data if isinstance(data, list) else data['entities']
        
        # 3. 定位修改
        for entity in entities:
            if entity['identifier'] == identifier:
                entity.update(updates)
                break
        
        # 4. 写回
        await f.seek(0)
        await f.write(json.dumps(data, ensure_ascii=False, indent=2))
        await f.truncate()
```

#### 2. 事务性保存

```python
import shutil
from datetime import datetime

async def save_with_backup(file_path: Path, data: dict):
    """保存时创建备份"""
    backup_path = file_path.with_suffix(
        f".{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak"
    )
    
    try:
        # 1. 创建备份
        shutil.copy2(file_path, backup_path)
        
        # 2. 写入新数据
        async with aiofiles.open(file_path, 'w') as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))
        
        # 3. 验证写入
        async with aiofiles.open(file_path, 'r') as f:
            json.loads(await f.read())  # 验证JSON合法
        
        # 4. 删除备份（或保留n个）
        cleanup_old_backups(file_path, keep=5)
        
    except Exception as e:
        # 回滚
        shutil.move(backup_path, file_path)
        raise e
```

#### 3. 批量操作优化

```python
async def batch_update_entities(updates: List[dict]):
    """批量更新多个实体"""
    # 按文件分组
    grouped = {}
    for update in updates:
        file = find_entity_file(update['subject'], update['id'])
        grouped.setdefault(file, []).append(update)
    
    # 每个文件只读写一次
    tasks = []
    for file_path, file_updates in grouped.items():
        tasks.append(update_file_batch(file_path, file_updates))
    
    await asyncio.gather(*tasks)
```

## 四、监控和调优

### 性能监控

```python
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start = time.time()
        result = await func(*args, **kwargs)
        duration = time.time() - start
        
        # 记录慢查询
        if duration > 1.0:
            logger.warning(f"慢查询: {func.__name__} 耗时 {duration:.2f}s")
        
        return result
    return wrapper

@router.get("/entities")
@monitor_performance
async def get_entities(...):
    pass
```

### 资源限制

```python
from fastapi import FastAPI
from fastapi_limiter import FastAPILimiter
import aioredis

# 限流配置
@app.on_event("startup")
async def startup():
    redis = await aioredis.create_redis_pool("redis://localhost")
    await FastAPILimiter.init(redis)

# API限流
@router.get("/entities")
@limiter.limit("100/minute")  # 每分钟100次
async def get_entities(...):
    pass
```

## 五、实施建议

### 阶段1：基础优化（立即实施）
1. ✅ 配置数据库连接池
2. ✅ 添加内存缓存
3. ✅ 优化分页查询
4. ✅ 添加性能监控

### 阶段2：异步改造（1-2周）
1. 迁移到异步数据库操作
2. 改造所有API为异步
3. 实现异步文件I/O
4. 压力测试和调优

### 阶段3：高级特性（2-4周）
1. 集成Redis缓存
2. 实时图谱可视化
3. 数据编辑入库
4. WebSocket实时推送

### 预期效果

| 阶段 | 并发能力 | 响应时间 | 实施难度 |
|------|---------|---------|---------|
| 当前 | 10 req/s | 200ms | - |
| 阶段1 | 50 req/s | 100ms | 低 |
| 阶段2 | 200 req/s | 50ms | 中 |
| 阶段3 | 500+ req/s | 20ms | 高 |

## 六、依赖更新

```txt
# requirements.txt 新增
aiosqlite>=0.19.0        # SQLite异步
aiomysql>=0.2.0          # MySQL异步
aiofiles>=23.2.0         # 异步文件I/O
aioredis>=2.0.0          # Redis异步（可选）
fastapi-limiter>=0.1.0   # API限流（可选）
```
